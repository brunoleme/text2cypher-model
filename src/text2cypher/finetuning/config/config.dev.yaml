project_name: "notechat-finetune"

logging:
  log_path: "logs"

model:
  type: "t5"
  name: "t5-small"
  quantization: false
  peft_method: "lora"
  max_length: 512

lora:
  r: 8
  alpha: 8
  dropout: 0.1
  prompt_tuning_n_tokens: 20

training:
  batch_size: 4
  learning_rate: 2e-5
  warmup_steps: 500
  weight_decay: 0.01
  patience: 3
  gradient_clip_val: 1.0
  max_epochs: 5
  num_workers: 4
  checkpoint_dir: "checkpoints"
  devices: "auto"

data:
  train_samples: 20  # Use -1 for all training data
  val_samples: 10     # Use -1 for all validation data
  test_samples: 10    # Use -1 for all test data
  shuffle: false        # Whether to shuffle datasets
  shuffle_seed: 42      # Random seed for reproducibility
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  shuffle_buffer_size: 5000
  input_folder_uri: "src/text2cypher/finetuning/data"
  sagemaker_run: true

evaluation:
  test_samples_lexical_metrics: 16
  test_samples_semantic_metrics: 16
  test_samples_ai_as_judge_metrics: 8

evaluation_model:
  checkpoint: "checkpoints/best-t5-small-peftlora-20250711-143428-epoch04-val_loss4.43.ckpt"
  display_name: "t5-small-peftlora"