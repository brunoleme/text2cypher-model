project_name: "notechat-finetune"

logging:
  log_path: "logs"

model:
  type: "t5"
  name: "t5-small"
  quantization: false
  peft_method: "lora"
  max_length: 512

lora:
  r: 8
  alpha: 8
  dropout: 0.1
  prompt_tuning_n_tokens: 20

training:
  batch_size: 4
  learning_rate: 2e-5
  warmup_steps: 500
  weight_decay: 0.01
  patience: 3
  gradient_clip_val: 1.0
  max_epochs: 5
  num_workers: 4
  model_artifact_dir: "/opt/ml/processing/output/model-artifacts"
  devices: "auto"

data:
  train_samples: 20  # Use -1 for all training data
  val_samples: 10     # Use -1 for all validation data
  test_samples: 10    # Use -1 for all test data
  shuffle: false        # Whether to shuffle datasets
  shuffle_seed: 42      # Random seed for reproducibility
  train_split: 0.5
  val_split: 0.25
  test_split: 0.25
  shuffle_buffer_size: 5000
  source_data_folder: "/opt/ml/processing/input/source-data"
  source_data_path: "notechat_dataset.csv"
  preprocessed_output_data_folder: "/opt/ml/processing/output"
  preprocessed_input_data_folder: "/opt/ml/processing/input"

evaluation:
  reports_dir: "/opt/ml/processing/output/reports"
  model_artifact_dir: "/opt/ml/processing/input/model-artifacts"
  test_samples_lexical_metrics: 16
  test_samples_semantic_metrics: 16
  test_samples_ai_as_judge_metrics: 8
  